### **Myriad Cognitive Architecture - The "Brain Approach" Evolution Roadmap**

**Preamble:** This roadmap outlines the long-term architectural evolution required to transition the Myriad system from using pre-trained embedding models for semantic understanding to a state where "meaning" is an emergent property of the entire agent network. This is the path to true biomimicry.

---

### **Phase 1: Foundational Infrastructure - The "Neural Substrate"**

**Goal:** To replace the current memory and communication backbone with a system that can support a dynamic, interconnected knowledge graph.

*   **Step 1.1: Implement the Graph Database Core**
    *   **Task 1.1.1:** Integrate a graph database (e.g., Neo4j, ArangoDB, or a cloud-native equivalent) into the system via `docker-compose.yml`.
    *   **Task 1.1.2:** Define the core graph schema as outlined in `PROTOCOLS.md (Section 3.1)`. This includes `AgentNode` properties and `COLLABORATES_WITH` relationship properties.
    *   **Task 1.1.3:** Create a new `GraphDB_Manager_AI` service. This Genesis Agent will be the sole interface for all read/write operations to the graph, ensuring consistency and centralizing logic.

*   **Step 1.2: Rearchitect the Orchestrator as a "Graph Traverser"**
    *   **Task 1.2.1:** Modify `orchestration/orchestrator.py`. Remove the direct `requests.post` calls to agent microservices for knowledge retrieval.
    *   **Task 1.2.2:** The orchestrator's primary function becomes query parsing followed by graph traversal. It will receive a query, identify the starting `ConceptNode`(s), and then query the `GraphDB_Manager_AI` to find the most relevant, strongly connected nodes.
    *   **Task 1.2.3:** The "result" of an orchestration is no longer a collection of JSON responses, but a **subgraph** of activated nodes and their relationships.

*   **Step 1.3: Migrate Existing Agents to Graph Nodes**
    *   **Task 1.3.1:** Write a migration script. This script will read the capabilities of the existing `Lightbulb_Definition_AI` and `Lightbulb_Function_AI`.
    *   **Task 1.3.2:** The script will use the `GraphDB_Manager_AI` to create `AgentNode`s in the graph that *represent* these agents. The node properties will contain their capabilities (intents, concepts) and their network endpoints.
    *   **Note:** In this phase, `Function-Executor` agents (Type B) will still exist as microservices, but they will be activated based on graph lookups, not a simple registry. `Fact-Base` agents (Type A) will be fully replaced by the graph itself.

**Phase 1 Deliverable:** A system where all knowledge and agent discovery is mediated by a central graph database. The `Orchestrator` can answer a query like "What is a lightbulb?" by traversing the graph, finding the `Lightbulb_Definition_AI` node, and then calling its endpoint.

---

### **Phase 2: Sensory Grounding & Associative Learning - The "Hippocampus"**

**Goal:** To implement the core learning mechanism where the system can form new connections between concepts based on sensory input.

*   **Step 2.1: Implement the Primal Sensory Cortex**
    *   **Task 2.1.1:** Build and deploy the `Image_Embedding_AI` and `Audio_Embedding_AI` as planned. These are the system's "eyes and ears."
    *   **Task 2.1.2:** Enhance the graph schema to include `SensoryNode` types for storing image and audio vectors.

*   **Step 2.2: Implement the `Learning_Engine_AI`**
    *   **Task 2.2.1:** Create the `Learning_Engine_AI` service. This is a core Genesis Agent.
    *   **Task 2.2.2:** Implement the `POST /learn/associate` endpoint. This endpoint will receive a learning event payload (e.g., a concept, an image, and a related textual fact).

*   **Step 2.3: Implement the Associative Learning Process**
    *   **Task 2.3.1:** When the `Learning_Engine_AI` receives a learning event, it will orchestrate the entire process:
        1.  Create a `ConceptNode` for the new concept (if it doesn't exist).
        2.  Call the sensory agents (`Image_Embedding_AI`, etc.) to get vectors for the sensory data.
        3.  Create corresponding `SensoryNode`s in the graph.
        4.  Create new, weighted `Relationship` edges in the graph connecting the `ConceptNode` to its sensory representations and other related concepts. For example: `(Dog) -[:HAS_VISUAL]-> (ImageVector123)`.

**Phase 2 Deliverable:** A system that can learn in a brain-like way. You can teach it the concept of "cat" by providing the word, a few pictures, and the sentence "a cat is a pet." The system will create a "cat" node and form weighted connections to the relevant image vectors and the existing "pet" node, creating its own distributed representation.

---

### **Phase 3: Emergent Semantics & Hebbian Learning - The "Cortex"**

**Goal:** To make the system's understanding of meaning truly dynamic by strengthening connections based on usage, allowing the knowledge graph to self-organize.

*   **Step 3.1: Implement the `Collaboration_Monitor_AI`**
    *   **Task 3.1.1:** Create a new `Collaboration_Monitor_AI` service.
    *   **Task 3.1.2:** After every successful query, the `Orchestrator` will send the activated subgraph to this monitor.
    *   **Task 3.1.3:** The monitor's job is to analyze which nodes were activated *together* to satisfy the query.

*   **Step 3.2: Implement Hebbian Learning**
    *   **Task 3.2.1:** The `Collaboration_Monitor` will send a request to the `GraphDB_Manager_AI` to update the graph.
    *   **Task 3.2.2:** The manager will execute the Hebbian learning rule: for every pair of nodes that were co-activated, it will slightly increase the `weight` and `activation_frequency` properties on the relationship edge connecting them. "Agents that fire together, wire together."

*   **Step 3.3: Evolve Graph Traversal Logic**
    *   **Task 3.3.1:** Modify the `Orchestrator`'s traversal queries to prioritize paths with higher weights.
    *   **Task 3.3.2:** This creates a feedback loop: frequently used pathways become stronger, and stronger pathways are more likely to be used in the future, mimicking the formation of strong neural pathways in the brain.

**Phase 3 Deliverable:** A self-optimizing cognitive architecture. As the system answers more queries, it will automatically learn common associations. The connection between "lightbulb" and "factories" will become stronger over time, making queries about their relationship faster and more accurate. The system's intelligence is now truly emergent and adaptive.

---

### **Phase 4: Autonomous Refinement & Pruning - The "Sleep Cycle"**

**Goal:** To implement background processes for memory consolidation, optimization, and the removal of weak or irrelevant connections.

*   **Step 4.1: Implement the `Consolidator_AI` (The "Sleep" Agent)**
    *   **Task 4.1.1:** Create the `Consolidator_AI` as a background worker.
    *   **Task 4.1.2:** During idle periods, this agent will perform maintenance on the knowledge graph.

*   **Step 4.2: Implement Synaptic Pruning**
    *   **Task 4.2.1:** The `Consolidator` will periodically scan the graph for relationship edges with very low weights and low activation frequencies.
    *   **Task 4.2.2:** These weak, unused connections will be removed from the graph. This is analogous to synaptic pruning in the brain, which improves cognitive efficiency by removing "noise."

*   **Step 4.3: Implement Higher-Order Concept Formation**
    *   **Task 4.3.1:** The `Consolidator` will also look for "cliques" or clusters of highly interconnected nodes.
    *   **Task 4.3.2:** When a dense cluster is found, it can create a new, higher-order `ConceptNode` that represents the entire cluster, forming an abstraction. For example, if it finds that "dog," "cat," and "hamster" are all strongly connected to "pet," it might create a new concept called "domestic_animals."

**Phase 4 Deliverable:** A mature and efficient cognitive system. It not only learns and adapts but also actively refines its own knowledge structure, improving its performance, removing irrelevant information, and forming abstract concepts autonomously. This completes the core framework for the "Brain Approach."